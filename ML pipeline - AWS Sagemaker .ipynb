{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "16b7e9bd-dd4f-45ea-a1c3-409dd6df84d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Machine Learning Model - Cloud Computing and AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b75d8b70-2a34-455a-8ff5-f94b8605696c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preparation\n",
    "\n",
    "\n",
    "\n",
    "- In the event it shuts down, but you still have your notebook open, you may copy your notebook cell by cell to another notebook, say in your own juyter notebook environment. That way, you at least save your scripts without the output.\n",
    "\n",
    "\n",
    "Using the following code to create a Spark Session so that you can use it subsequently for query and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ab8dd75-c493-4ff3-9179-352d686c6ce8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ec2-user/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ec2-user/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0e2eb87d-88d7-4a99-a1b3-78654279e2e8;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.563 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.2/hadoop-aws-3.2.2.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.2.2!hadoop-aws.jar (40ms)\n",
      "downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar ...\n",
      "\t[SUCCESSFUL ] com.amazonaws#aws-java-sdk-bundle;1.11.563!aws-java-sdk-bundle.jar (1691ms)\n",
      ":: resolution report :: resolve 2961ms :: artifacts dl 1737ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.563 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   2   |   2   |   0   ||   2   |   2   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0e2eb87d-88d7-4a99-a1b3-78654279e2e8\n",
      "\tconfs: [default]\n",
      "\t2 artifacts copied, 0 already retrieved (127385kB/105ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/20 03:23:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-16-155-129.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0b0ddd5840>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"PySparkApp\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import warnings, requests, zipfile, io\n",
    "warnings.simplefilter('ignore')\n",
    "import pandas as pd\n",
    "from scipy.io import arff\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.image_uris import retrieve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "84bb1565-650b-4416-97a4-dec5a5ae0eb3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##  Using SageMaker to Train and Depoly a Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abalone dataset has been used to predict the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope - - a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location(hence food availability) may be required to solve the problem. More information about the dataset is at: https: // archive.ics.uci.edu/ml/datasets/abalone\n",
    "\n",
    "a copy of the data is available through sagemaker sample data files at `s3: // sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv`\n",
    "\n",
    "In the part, we use Spark to prepare the data locally, and then use a built-in SageMaker xgboost algorithm to train a XGBoost regression model and then deploy it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. First download the dataset using `aws s3 cp s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv  abalone.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv to ./abalone.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/uci_abalone/abalone.csv abalone.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. In the following, load abalone.csv to a Spark DataFrame, using the schema provided below.\n",
    "\n",
    "```\n",
    "schema = \"sex String,length Double,diameter Double,height Double,whole_weight Double,shucked_weight Double,viscera_weight Double,shell_weight Double,rings Double\"\n",
    "```\n",
    "\n",
    "- Then, view 10 rows from this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/20 03:24:35 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: M, 0.455, 0.365, 0.095, 0.514, 0.2245, 0.101, 0.15, 15\n",
      " Schema: sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings\n",
      "Expected: sex but found: M\n",
      "CSV file: file:///home/ec2-user/SageMaker/abalone.csv\n",
      "+---+------+--------+------+------------+--------------+--------------+------------+-----+\n",
      "|sex|length|diameter|height|whole_weight|shucked_weight|viscera_weight|shell_weight|rings|\n",
      "+---+------+--------+------+------------+--------------+--------------+------------+-----+\n",
      "|  M|  0.35|   0.265|  0.09|      0.2255|        0.0995|        0.0485|        0.07|  7.0|\n",
      "|  F|  0.53|    0.42| 0.135|       0.677|        0.2565|        0.1415|        0.21|  9.0|\n",
      "|  M|  0.44|   0.365| 0.125|       0.516|        0.2155|         0.114|       0.155| 10.0|\n",
      "|  I|  0.33|   0.255|  0.08|       0.205|        0.0895|        0.0395|       0.055|  7.0|\n",
      "|  I| 0.425|     0.3| 0.095|      0.3515|         0.141|        0.0775|        0.12|  8.0|\n",
      "|  F|  0.53|   0.415|  0.15|      0.7775|         0.237|        0.1415|        0.33| 20.0|\n",
      "|  F| 0.545|   0.425| 0.125|       0.768|         0.294|        0.1495|        0.26| 16.0|\n",
      "|  M| 0.475|    0.37| 0.125|      0.5095|        0.2165|        0.1125|       0.165|  9.0|\n",
      "|  F|  0.55|    0.44|  0.15|      0.8945|        0.3145|         0.151|        0.32| 19.0|\n",
      "|  F| 0.525|    0.38|  0.14|      0.6065|         0.194|        0.1475|        0.21| 14.0|\n",
      "+---+------+--------+------+------------+--------------+--------------+------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"sex\", StringType(), True),\n",
    "    StructField(\"length\", DoubleType(), True),\n",
    "    StructField(\"diameter\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True),\n",
    "    StructField(\"whole_weight\", DoubleType(), True),\n",
    "    StructField(\"shucked_weight\", DoubleType(), True),\n",
    "    StructField(\"viscera_weight\", DoubleType(), True),\n",
    "    StructField(\"shell_weight\", DoubleType(), True),\n",
    "    StructField(\"rings\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(\"/home/ec2-user/SageMaker/abalone.csv\", schema=schema, header=True)\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Build a data engineering pipeline to prepare the data:\n",
    "- Sex is converted to numeric values via StringIndexer\n",
    "- the first column is `rings` (i.e. age of abalone) because SageMaker algorithm requires the first column of the training data to be label.\n",
    "- keep the indexed sex column but not the original sex column.\n",
    "- keep the remaining columns.\n",
    "- randomly split the dataframe into `train_df` (80%) and `validation_df` (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62fffdca-41a5-4fdf-a97c-af7f417ddf74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/20 03:25:54 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: M\n",
      " Schema: sex\n",
      "Expected: sex but found: M\n",
      "CSV file: file:///home/ec2-user/SageMaker/abalone.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# Load the dataset and define the schema\n",
    "# ... (load the data as shown previously)\n",
    "\n",
    "# Create a StringIndexer to convert 'sex' column to numeric values\n",
    "indexer = StringIndexer(inputCol=\"sex\", outputCol=\"sex_indexed\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=[indexer])\n",
    "\n",
    "# Fit the pipeline to the dataset and transform the data\n",
    "model = pipeline.fit(df)\n",
    "transformed_df = model.transform(df)\n",
    "\n",
    "# Reorder the columns\n",
    "required_columns = [\"rings\"] + [col for col in df.columns if col not in [\"rings\", \"sex\"]] + [\"sex_indexed\"]\n",
    "final_df = transformed_df.select(required_columns)\n",
    "\n",
    "# Split the DataFrame\n",
    "train_df, validation_df = final_df.randomSplit([0.8, 0.2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/20 03:26:02 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: M, 0.455, 0.365, 0.095, 0.514, 0.2245, 0.101, 0.15, 15\n",
      " Schema: sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings\n",
      "Expected: sex but found: M\n",
      "CSV file: file:///home/ec2-user/SageMaker/abalone.csv\n",
      "+-----+------+--------+------+------------+--------------+--------------+------------+-----------+\n",
      "|rings|length|diameter|height|whole_weight|shucked_weight|viscera_weight|shell_weight|sex_indexed|\n",
      "+-----+------+--------+------+------------+--------------+--------------+------------+-----------+\n",
      "|  2.0|  0.15|     0.1| 0.025|       0.015|        0.0045|         0.004|       0.005|        1.0|\n",
      "|  3.0|  0.13|     0.1|  0.03|       0.013|        0.0045|         0.003|       0.004|        1.0|\n",
      "|  3.0|  0.14|   0.105| 0.035|       0.014|        0.0055|        0.0025|       0.004|        1.0|\n",
      "|  3.0| 0.155|    0.11|  0.04|      0.0155|        0.0065|         0.003|       0.005|        0.0|\n",
      "|  3.0|  0.16|    0.11| 0.025|       0.018|        0.0065|        0.0055|       0.005|        1.0|\n",
      "+-----+------+--------+------+------------+--------------+--------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Write dataframes as csv:\n",
    "- Write `train_df` and `validation_df` to local directories `train` and `validation` respectively, using the csv format.\n",
    "- Also write a copy of validation_df without the `rings` column to a local directory `test`, also using the csv format.\n",
    "- verify the content of these files using bash commands, e.g. `head train/*.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_df = validation_df.drop(\"rings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/20 03:26:16 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: M, 0.455, 0.365, 0.095, 0.514, 0.2245, 0.101, 0.15, 15\n",
      " Schema: sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings\n",
      "Expected: sex but found: M\n",
      "CSV file: file:///home/ec2-user/SageMaker/abalone.csv\n"
     ]
    }
   ],
   "source": [
    "# Write train_df to a local directory in CSV format\n",
    "train_df.write.csv(\"/home/ec2-user/SageMaker/train/\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rings,length,diameter,height,whole_weight,shucked_weight,viscera_weight,shell_weight,sex_indexed\n",
      "2.0,0.15,0.1,0.025,0.015,0.0045,0.004,0.005,1.0\n",
      "3.0,0.13,0.1,0.03,0.013,0.0045,0.003,0.004,1.0\n",
      "3.0,0.14,0.105,0.035,0.014,0.0055,0.0025,0.004,1.0\n",
      "3.0,0.155,0.11,0.04,0.0155,0.0065,0.003,0.005,0.0\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 train/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/20 03:26:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: M, 0.455, 0.365, 0.095, 0.514, 0.2245, 0.101, 0.15, 15\n",
      " Schema: sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings\n",
      "Expected: sex but found: M\n",
      "CSV file: file:///home/ec2-user/SageMaker/abalone.csv\n",
      "23/11/20 03:26:22 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: M, 0.455, 0.365, 0.095, 0.514, 0.2245, 0.101, 0.15, 15\n",
      " Schema: sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings\n",
      "Expected: sex but found: M\n",
      "CSV file: file:///home/ec2-user/SageMaker/abalone.csv\n"
     ]
    }
   ],
   "source": [
    "validation_df.write.csv(\"/home/ec2-user/SageMaker/validation/\", header=True)\n",
    "test_df.write.csv(\"/home/ec2-user/SageMaker/test/\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rings,length,diameter,height,whole_weight,shucked_weight,viscera_weight,shell_weight,sex_indexed\n",
      "3.0,0.13,0.1,0.03,0.013,0.0045,0.003,0.004,1.0\n",
      "3.0,0.18,0.125,0.05,0.023,0.0085,0.0055,0.01,0.0\n",
      "3.0,0.18,0.13,0.045,0.0275,0.0125,0.01,0.009,1.0\n",
      "3.0,0.195,0.15,0.045,0.0375,0.018,0.006,0.011,1.0\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 validation/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length,diameter,height,whole_weight,shucked_weight,viscera_weight,shell_weight,sex_indexed\n",
      "0.13,0.1,0.03,0.013,0.0045,0.003,0.004,1.0\n",
      "0.18,0.125,0.05,0.023,0.0085,0.0055,0.01,0.0\n",
      "0.18,0.13,0.045,0.0275,0.0125,0.01,0.009,1.0\n",
      "0.195,0.15,0.045,0.0375,0.018,0.006,0.011,1.0\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 test/*.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Run the following to obtain prefix and bucket names, then copy csv files (for training, validation, and test) to corresponding folders on s3.\n",
    "\n",
    "e.g. the following command copy your training csv to s3\n",
    "\n",
    "`aws s3 cp train/*.csv s3://$bucket/$prefix/train/train.csv`\n",
    "\n",
    "- then verify your uploaded file using aws s3 cp and head combined (via pipe)\n",
    "\n",
    "`aws s3 cp s3://$bucket/$prefix/train/train.csv - | head`\n",
    "\n",
    "(you can ignore the \"broken pipe\" error; the command still works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "prefix = 'xgboost-builtin-algo'\n",
    "bucket = sagemaker.Session().default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: train/part-00000-e1a61c67-d43f-48ba-9797-32a5022b6a5a-c000.csv to s3://sagemaker-us-east-1-380875404404/xgboost-builtin-algo/train/train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp train/*.csv s3://$bucket/$prefix/train/train.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rings,length,diameter,height,whole_weight,shucked_weight,viscera_weight,shell_weight,sex_indexed\n",
      "2.0,0.15,0.1,0.025,0.015,0.0045,0.004,0.005,1.0\n",
      "3.0,0.13,0.1,0.03,0.013,0.0045,0.003,0.004,1.0\n",
      "3.0,0.14,0.105,0.035,0.014,0.0055,0.0025,0.004,1.0\n",
      "3.0,0.155,0.11,0.04,0.0155,0.0065,0.003,0.005,0.0\n",
      "3.0,0.16,0.11,0.025,0.018,0.0065,0.0055,0.005,1.0\n",
      "3.0,0.165,0.12,0.03,0.0215,0.007,0.005,0.005,1.0\n",
      "3.0,0.165,0.12,0.05,0.021,0.0075,0.0045,0.014,1.0\n",
      "3.0,0.18,0.125,0.05,0.023,0.0085,0.0055,0.01,0.0\n",
      "3.0,0.18,0.13,0.045,0.0275,0.0125,0.01,0.009,1.0\n",
      "download failed: s3://sagemaker-us-east-1-380875404404/xgboost-builtin-algo/train/train.csv to - [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$prefix/train/train.csv - | head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: validation/part-00000-39b75db2-57b6-48a6-bce4-fc61001703b7-c000.csv to s3://sagemaker-us-east-1-380875404404/xgboost-builtin-algo/validation/validation.csv\n",
      "upload: test/part-00000-540320d9-1665-4bda-9da5-968b7cdf34ad-c000.csv to s3://sagemaker-us-east-1-380875404404/xgboost-builtin-algo/test/test.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp validation/*.csv s3://$bucket/$prefix/validation/validation.csv\n",
    "!aws s3 cp test/*.csv s3://$bucket/$prefix/test/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1c85f1e-bff9-480e-9d90-e945d40be394",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "6\\. Train a XGBoost model using the training and validation datasets using SageMaker's built-in xgboost algorithm. \n",
    "\n",
    "- Create a `xgboost-container` \n",
    "    - `sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"1.5-1\")` will retrive the image (for xgboost version 1.5-1, the version is required) for using with the container.\n",
    "- Using the folloing hyperparameters (note that the objective `reg:squarederror` indicates that we use xgboost for numerical prediction with Mean Squared Error as evaluation metrics. \n",
    "\n",
    "        \"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"50\"\n",
    "\n",
    "- construct a SageMaker `estimator` using the `xgboost-container`, using the following configurations:\n",
    "    \n",
    "        sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                      hyperparameters=hyperparameters,\n",
    "                                      role=sagemaker.get_execution_role(),\n",
    "                                      instance_count=1, \n",
    "                                      instance_type='ml.m4.xlarge', \n",
    "                                      output_path=output_path)  \n",
    "\n",
    "\n",
    "- build two input sources `train_input`, `validation_input` using the `TrainInput` function \n",
    "    - TrainingInput(s3_folder, content_type='text/csv')`  returns a csv input channel. The build-in XGBoost algorithm of SageMaker only works with csv and libsvm formats. \n",
    "    - s3_folder (in the form of s3://bucket/prefix/folder) should be the folder you upload your data to. One example is `s3://sagemaker-us-east-1-308934269464/xgboost-builtin-algo/train/`\n",
    "    \n",
    "- Train the xgboost `estimator` using both train and validation inputs. This will take several minutes.\n",
    "\n",
    "More details at: https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f9b107a-4e16-431c-8b2b-7a1b63070ca0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", boto3.Session().region_name, \"1.5-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams={\"max_depth\":\"5\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"50\"\n",
    "}\n",
    "\n",
    "s3_output_location=\"s3://{}/{}/output/\".format(bucket,prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "  xgb_model =      sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                      hyperparameters=hyperparams,\n",
    "                                      role=sagemaker.get_execution_role(),\n",
    "                                      instance_count=1, \n",
    "                                      instance_type='ml.m4.xlarge', \n",
    "                                      output_path=s3_output_location) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-380875404404'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# Define the S3 paths to the training and validation data\n",
    "s3_training_data_path = 's3://sagemaker-us-east-1-380875404404/xgboost-builtin-algo/train/'\n",
    "s3_validation_data_path = 's3://sagemaker-us-east-1-380875404404/xgboost-builtin-algo/validation/'\n",
    "\n",
    "train_input = TrainingInput(s3_data=s3_training_data_path, content_type='text/csv')\n",
    "\n",
    "\n",
    "validation_input = TrainingInput(s3_data=s3_validation_data_path, content_type='text/csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {'train': train_input, 'validation': validation_input}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2023-11-20 03:27:09 Starting - Starting the training job...\n",
      "2023-11-20 03:27:33 Starting - Preparing the instances for training...............\n",
      "2023-11-20 03:28:55 Downloading - Downloading input data...........\n",
      "2023-11-20 03:29:55 Training - Downloading the training image...\n",
      "2023-11-20 03:30:15 Training - Training image download completed. Training in progress....\n",
      "2023-11-20 03:30:31 Uploading - Uploading generated training model..\n",
      "2023-11-20 03:30:50 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "xgb_model.fit(inputs=data_channels, logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e3ec99-032b-4854-8112-156f67145b01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "7\\. Deploy the trained model to an HTTP endpoint  (save as variable `predictor`)\n",
    "\n",
    "Using the following deployment parameters:\n",
    "\n",
    "   - initial_instance_count=1, \n",
    "   - instance_type=\"ml.m4.xlarge\"\n",
    "   \n",
    "This will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23e4d5f2-8b10-440c-a952-4bdd123c5839",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = xgb_model.deploy(initial_instance_count=1,\n",
    "                serializer = sagemaker.serializers.CSVSerializer(),\n",
    "                instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Test the deployment endpoint\n",
    "\n",
    "- read one line from the test data. Note that it should not have the label column and should be in the same csv format.\n",
    "- past the line to the end point using code such as this where \n",
    "    - `predictor` is the varilable returned from deploy() in the previous step. \n",
    "    - `payload` is the string-type input data (csv formatted).\n",
    "    \n",
    "The response from the end point is json type with body containing the predicted value. \n",
    "\n",
    "\n",
    "```\n",
    "runtime_client = sagemaker_session.sagemaker_runtime_client\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name, ContentType=\"text/csv\", Body=payload\n",
    ")\n",
    "result = response[\"Body\"].read().decode(\"ascii\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>diameter</th>\n",
       "      <th>height</th>\n",
       "      <th>whole_weight</th>\n",
       "      <th>shucked_weight</th>\n",
       "      <th>viscera_weight</th>\n",
       "      <th>shell_weight</th>\n",
       "      <th>sex_indexed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.075</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  diameter  height  whole_weight  shucked_weight  viscera_weight  \\\n",
       "0   0.075     0.055    0.01         0.002           0.001          0.0005   \n",
       "\n",
       "   shell_weight  sex_indexed  \n",
       "0        0.0015          1.0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_row_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/11/20 03:47:06 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: M, 0.455, 0.365, 0.095, 0.514, 0.2245, 0.101, 0.15, 15\n",
      " Schema: sex, length, diameter, height, whole_weight, shucked_weight, viscera_weight, shell_weight, rings\n",
      "Expected: sex but found: M\n",
      "CSV file: file:///home/ec2-user/SageMaker/abalone.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "first_row_dict = test_df.head(1)[0].asDict()\n",
    "first_row_df = pd.DataFrame([first_row_dict])\n",
    "\n",
    "payload = first_row_df.to_csv(header=False, index=False).strip('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "Predicted value: 3.395925521850586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already deployed your model and have the predictor\n",
    "sagemaker_session = sagemaker.Session()\n",
    "runtime_client = sagemaker_session.sagemaker_runtime_client\n",
    "\n",
    "# Invoking the endpoint\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name, \n",
    "    ContentType=\"text/csv\", \n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "# Decoding the response\n",
    "result = response[\"Body\"].read().decode(\"ascii\")\n",
    "print(\"Predicted value:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. (optional challenge) Extending the previous step by reading from validation dataset (so you know the ground truth)\n",
    "\n",
    "- read 10 rows from the validation csv.\n",
    "- split it so that you obtain the label as well as the string-type payload as input for the endpoint.\n",
    "- feed each payload to the predictor, obtain the result and print both the label and the predicted value side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"s3://{}/{}/validation/validation.csv\".format(bucket,prefix), nrows = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = data.columns.to_list()\n",
    "cols.remove(\"rings\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols\n",
    "val_data = data[[\"rings\"]]\n",
    "no_target_data = data[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data[\"pred_rings\"] = \"\"\n",
    "for index, row in no_target_data.iterrows():\n",
    "    val_data.loc[index, \"pred_rings\"] = float(predictor.predict(row.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rings</th>\n",
       "      <th>pred_rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.395926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.395926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.395926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.537944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.689819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.623317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rings pred_rings\n",
       "0    1.0   3.395926\n",
       "1    3.0   3.395926\n",
       "2    3.0   4.333157\n",
       "3    3.0   4.333157\n",
       "4    4.0   3.395926\n",
       "5    4.0   4.537944\n",
       "6    4.0   3.689819\n",
       "7    4.0   4.333157\n",
       "8    4.0   4.333157\n",
       "9    4.0   4.623317"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleanup the endpoint\n",
    "\n",
    "When you’re done using the endpoint, please run the cell below to delete the hosted endpoint and avoid any additional charges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Using test folder on S3 as input for batch inference using the trained model. \n",
    "\n",
    "- output should go to a `testout` folder on s3 (with the same bucket and prefix)\n",
    "- Use the following transformer parameters:\n",
    "       instance_count=1,\n",
    "       instance_type='ml.m4.xlarge',\n",
    "       strategy='MultiRecord',\n",
    "       assemble_with='Line'\n",
    "       \n",
    "This will take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_X_file = 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_output = \"s3://{}/{}/testout/\".format(bucket,prefix)\n",
    "\n",
    "batch_pre_process_input = \"s3://{}/{}/test/test.csv\".format(bucket,prefix)\n",
    "batch_input = \"s3://{}/{}/testout/test.csv\".format(bucket,prefix)\n",
    "\n",
    "pd.read_csv(batch_pre_process_input).to_csv(batch_input, index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................................\n",
      "\u001b[34m[2023-11-20:04:52:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:10:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"POST /invocations HTTP/1.1\" 200 15299 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-11-20T04:52:17.612:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:10:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:10:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:10:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2023-11-20 04:52:11 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m[2023-11-20 04:52:11 +0000] [19] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35m[2023-11-20 04:52:11 +0000] [19] [INFO] Listening at: unix:/tmp/gunicorn.sock (19)\u001b[0m\n",
      "\u001b[35m[2023-11-20 04:52:11 +0000] [19] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/os.py:1023: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  return io.open(fd, *args, **kwargs)\u001b[0m\n",
      "\u001b[35m[2023-11-20 04:52:11 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[35m[2023-11-20 04:52:11 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2023-11-20 04:52:11 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2023-11-20 04:52:11 +0000] [28] [INFO] Booting worker with pid: 28\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Loading the model from /opt/ml/model/xgboost-model\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:13:INFO] Model objective : reg:squarederror\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2023-11-20:04:52:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"POST /invocations HTTP/1.1\" 200 15299 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:17:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2023-11-20:04:52:17:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.8/site-packages/xgboost/core.py:105: UserWarning: ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [20/Nov/2023:04:52:17 +0000] \"POST /invocations HTTP/1.1\" 200 15299 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2023-11-20T04:52:17.612:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize the transformer object\n",
    "xgb_transformer = xgb_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    strategy='MultiRecord',\n",
    "    assemble_with='Line',\n",
    "    output_path=batch_output\n",
    ")\n",
    "\n",
    "\n",
    "xgb_transformer.transform(\n",
    "    data=batch_input,\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "\n",
    "xgb_transformer.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. Inspect the s3 folder using the technique introduced in step 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_rings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.395926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.395926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.395926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.537944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.689819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.333157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.623317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pred_rings\n",
       "0    3.395926\n",
       "1    3.395926\n",
       "2    4.333157\n",
       "3    4.333157\n",
       "4    3.395926\n",
       "5    4.537944\n",
       "6    3.689819\n",
       "7    4.333157\n",
       "8    4.333157\n",
       "9    4.623317"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket=bucket, Key=\"{}/testout/{}\".format(prefix,'test.csv.out'))\n",
    "target_predicted = pd.read_csv(io.BytesIO(obj['Body'].read()),sep=',',names=['pred_rings'])\n",
    "target_predicted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-20 04:45:32      37904 test.csv\n",
      "2023-11-20 04:52:18      15299 test.csv.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls s3://$bucket/$prefix/testout/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.395925521850586\n",
      "3.395925521850586\n",
      "4.333157062530518\n",
      "4.333157062530518\n",
      "3.395925521850586\n",
      "4.5379438400268555\n",
      "3.6898193359375\n",
      "4.333157062530518\n",
      "4.333157062530518\n",
      "4.623316764831543\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$prefix/testout/test.csv.out - | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Clean up\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "homework6",
   "notebookOrigID": 1426158129998156,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
